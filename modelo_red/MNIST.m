clc, clearvars, close all

% Permite utilizar las funciones definidas en el folder anterior
ant_dir =  fullfile(pwd, '..');
addpath(genpath(ant_dir));

% Extrae los datos de entrenamiento y de prueba
dataFolder = fullfile(toolboxdir('nnet'),'nndemos','nndatasets','DigitDataset');
[train_imgs, train_labels, test_imgs, test_labels] = img_extractor(dataFolder);

%%

% Normalizacion de los datos
train_imgs = train_imgs ./ 255;
test_imgs = test_imgs ./ 255;

%%

salida = capa(784, 10, 'softmax');

M = modelo(train_imgs, {salida});

%%
epocas = 100;

E_SGD = entrenamiento_test(train_imgs, train_labels, M, ...
    'learningRate', 0.01, ...
    'lossFunction', 'cross entropy', ...
    'trainAlgorithm', 'SGD', ...
    'epochs', epocas, ...
    'optimizer', 'GD');

E_NAG = entrenamiento_test(train_imgs, train_labels, M, ...
    'learningRate', 0.1, ...
    'lossFunction', 'cross entropy', ...
    'batch_size', 50, ...
    'trainAlgorithm', 'MBGD', ...
    'epochs', epocas, ...
    'optimizer', 'NAG');
  
E_Momentum= entrenamiento_test(train_imgs, train_labels, M, ...
    'learningRate', 0.01, ...
    'lossFunction', 'cross entropy', ...
    'batch_size', 50, ...
    'trainAlgorithm', 'MBGD', ...
    'epochs', epocas, ...
    'optimizer', 'momentum');

E_RMSProp = entrenamiento_test(train_imgs, train_labels, M, ...
    'learningRate', 0.0005, ...
    'lossFunction', 'cross entropy', ...
    'trainAlgorithm', 'MBGD', ...
    'batch_size', 50, ...
    'epochs', epocas, ...
    'optimizer', 'RMSProp');

E_Adagrad = entrenamiento_test(train_imgs, train_labels, M, ...
    'learningRate', 0.004, ...
    'lossFunction', 'cross entropy', ...
    'trainAlgorithm', 'MBGD', ...
    'batch_size', 50, ...
    'epochs', epocas, ...
    'optimizer', 'Adagrad');


%%
acc_SGD = precision(test_imgs, test_labels, E_SGD);
acc_NAG = precision(train_imgs, train_labels, E_NAG);
acc_Momentum = precision(train_imgs, train_labels, E_Momentum);
acc_RMSProp = precision(train_imgs, train_labels, E_RMSProp);
acc_Adagrad = precision(train_imgs, train_labels, E_Adagrad);

figure()
plot(1:epocas, E_NAG.C, 1:epocas, E_SGD.C, 1:epocas, E_Momentum.C, 1:epocas, E_RMSProp.C, 1:epocas, E_Adagrad.C, 'LineWidth', 0.5);
legend(['Nesterov Accelerated Gradrient, acc: ', num2str(acc_NAG)], ...
            ['Stochastic Gradient Descent, acc: ', num2str(acc_SGD)], ...
            ['Gradient Descent with Momentum, acc: ', num2str(acc_Momentum)], ...
            ['Root Mean Squared Propagation, acc: ', num2str(acc_RMSProp)], ...
            ['Adaptative Gradient, acc: ', num2str(acc_Adagrad)]) 
            
            



%%
n = 4;
%pred = predictor(test_imgs(:,n), E_MBGD);
%num_pred = find(pred == max(pred)) - 1;
 

figure()
img = reshape(train_imgs(:,n), 28, 28);
imshow(img, 'Border', 'tight', 'InitialMagnification', 'fit')

display(train_labels(:,n));
